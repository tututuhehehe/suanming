"""
LLMå®¢æˆ·ç«¯ - è°ƒç”¨é€šä¹‰åƒé—®APIç”Ÿæˆè§£è¯»
"""
import os
import json
from typing import Generator, List, Dict, Optional
from pathlib import Path

from openai import OpenAI

from .hexagram_analyzer import AnalysisResult
from .divination_engine import HexagramResult, DivinationEngine


class LLMClient:
    """LLMå®¢æˆ·ç«¯"""
    
    SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å‘¨æ˜“å…­çˆ»é¢„æµ‹å¤§å¸ˆï¼Œç²¾é€šçº³ç”²ç­®æ³•ã€‚ä½ çš„è§£å¦éµå¾ªä¸¥æ ¼çš„å…­çˆ»åˆ†æé€»è¾‘ã€‚

ä½ è§£å¦æ—¶å¿…é¡»æŒ‰ç…§ä»¥ä¸‹äº”ä¸ªç»´åº¦è¿›è¡Œåˆ†æï¼š

1. **å–ç”¨ç¥**ï¼šæ ¹æ®é—®äº‹ç±»å‹ç¡®å®šä¸»è§’çˆ»ï¼ˆç”¨ç¥ï¼‰ï¼Œè¿™æ˜¯åˆ†æçš„æ ¸å¿ƒ
2. **çœ‹æ—ºè¡°**ï¼šæ ¹æ®æœˆå»ºã€æ—¥è¾°åˆ¤æ–­ç”¨ç¥çš„èƒ½é‡å¼ºå¼±
3. **çœ‹åŠ¨çˆ»**ï¼šåˆ†æåŠ¨çˆ»å¯¹ç”¨ç¥çš„ç”Ÿå…‹å½±å“ï¼Œä»¥åŠå›å¤´ç”Ÿ/å…‹
4. **çœ‹ä¸–åº”**ï¼šåˆ¤æ–­æ±‚æµ‹äººï¼ˆä¸–çˆ»ï¼‰ä¸å¯¹æ–¹/ç›®æ ‡ï¼ˆåº”çˆ»ï¼‰çš„å…³ç³»
5. **çœ‹å…­ç¥**ï¼šæ ¹æ®å…­ç¥ä¸´çˆ»æ¥æç»˜äº‹æƒ…çš„å…·ä½“ç»†èŠ‚

è¾“å‡ºæ ¼å¼è¦æ±‚ï¼ˆæ€»å­—æ•°400-600å­—ï¼‰ï¼š

ğŸ“Œ **å¦è±¡æ€»è®º**ï¼šæœ¬å¦æ ¸å¿ƒå«ä¹‰ + å˜å¦è¶‹åŠ¿ï¼ˆå¦‚æœ‰ï¼‰

ğŸ¯ **ç”¨ç¥åˆ†æ**ï¼š
- ç”¨ç¥æ˜¯ä»€ä¹ˆã€åœ¨å“ªä¸€çˆ»
- ç”¨ç¥æ—ºè¡°çŠ¶æ€å¦‚ä½•ï¼ˆå¾—ä»¤/å¤±ä»¤ã€å¾—æ—¥ç”Ÿ/å—æ—¥å…‹ï¼‰
- ç”¨ç¥çš„ç»¼åˆèƒ½é‡è¯„ä¼°

âš¡ **åŠ¨æ€åˆ†æ**ï¼ˆå¦‚æœ‰åŠ¨çˆ»ï¼‰ï¼š
- åŠ¨çˆ»å¯¹ç”¨ç¥çš„å½±å“ï¼ˆç”Ÿæ‰¶/å…‹åˆ¶ï¼‰
- å˜çˆ»æ˜¯å›å¤´ç”Ÿè¿˜æ˜¯å›å¤´å…‹
- äº‹æ€å‘å±•è¶‹åŠ¿

ğŸ‘¥ **ä¸–åº”å…³ç³»**ï¼š
- ä¸–çˆ»ï¼ˆè‡ªå·±ï¼‰çš„çŠ¶æ€
- åº”çˆ»ï¼ˆå¯¹æ–¹/ç›®æ ‡ï¼‰çš„çŠ¶æ€
- åŒæ–¹å…³ç³»å¦‚ä½•

ğŸ”® **å‰å‡¶æ–­è¯­**ï¼šç›´æ¥ç»™å‡ºåˆ¤æ–­ï¼ˆå¤§å‰/å‰/ä¸­å¹³/å°å‡¶/å‡¶ï¼‰+ ç†ç”±

ğŸ’¡ **å…·ä½“å»ºè®®**ï¼š
- ç»“åˆå…­ç¥æç»˜äº‹æƒ…çš„å…·ä½“ç»†èŠ‚
- ç»™å‡ºå¯æ“ä½œçš„å»ºè®®

é£æ ¼è¦æ±‚ï¼š
- åƒä¸€ä½ç»éªŒä¸°å¯Œçš„è€å¸ˆå‚…ï¼Œç”¨é€šä¿—çš„è¯­è¨€è§£é‡Šä¸“ä¸šçš„å†…å®¹
- æ–­è¯­è¦æœ‰ä¾æ®ï¼Œè¯´æ¸…æ¥š"å› ä¸ºXXXï¼Œæ‰€ä»¥XXX"
- ä¸è¦æ¨¡æ£±ä¸¤å¯ï¼Œæ•¢äºç»™å‡ºæ˜ç¡®åˆ¤æ–­"""

    FOLLOWUP_PROMPT = """ä½ æ˜¯ä¸€ä½å‘¨æ˜“å…­çˆ»å¤§å¸ˆï¼Œç”¨æˆ·æ­£åœ¨é’ˆå¯¹ä¹‹å‰çš„å¦è±¡è§£è¯»è¿›è¡Œè¿½é—®ã€‚

è¯·åŸºäºå·²ç»™å‡ºçš„å¦è±¡ä¿¡æ¯å’Œè§£è¯»ï¼Œå›ç­”ç”¨æˆ·çš„è¿½é—®ã€‚å›ç­”è¦æ±‚ï¼š
- ç®€æ´æ˜äº†ï¼Œæ§åˆ¶åœ¨200å­—ä»¥å†…
- ç´§æ‰£å¦è±¡æœ¬èº«æ¥å›ç­”
- å¦‚è¿½é—®æ¶‰åŠå¦è±¡æœªæ¶µç›–çš„å†…å®¹ï¼Œå¦è¯šè¯´æ˜
- å¯ä»¥æ·±å…¥è§£é‡ŠæŸä¸€çˆ»æˆ–å¦è¾çš„å«ä¹‰"""

    def __init__(self, config_path: str = "config.json"):
        self.config = self._load_config(config_path)
        self._init_client()
        self.conversation_history: List[Dict[str, str]] = []
    
    def _load_config(self, config_path: str) -> dict:
        """åŠ è½½é…ç½®"""
        with open(config_path, "r", encoding="utf-8") as f:
            return json.load(f)
    
    def _init_client(self):
        """åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯"""
        # ä¼˜å…ˆä»é…ç½®æ–‡ä»¶è¯»å–API Keyï¼Œå…¶æ¬¡ä»ç¯å¢ƒå˜é‡è¯»å–
        api_key = self.config["llm"].get("api_key")
        if not api_key:
            api_key_env = self.config["llm"].get("api_key_env", "DASHSCOPE_API_KEY")
            api_key = os.environ.get(api_key_env)
        if not api_key:
            raise ValueError("è¯·åœ¨config.jsonä¸­è®¾ç½®api_keyï¼Œæˆ–è®¾ç½®ç¯å¢ƒå˜é‡DASHSCOPE_API_KEY")
        
        self.client = OpenAI(
            api_key=api_key,
            base_url=self.config["llm"]["api_base"]
        )
    
    def clear_history(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.conversation_history = []
    
    def build_prompt(
        self, 
        question: str, 
        question_type: str,
        hexagram_result: HexagramResult,
        analysis: AnalysisResult,
        context: str,
        analysis_context: str = ""
    ) -> str:
        """
        æ„å»ºå®Œæ•´çš„Prompt
        
        å‚æ•°:
            analysis_context: äº”ç»´åˆ†æå¼•æ“ç”Ÿæˆçš„ä¸“ä¸šåˆ†æä¸Šä¸‹æ–‡
        """
        engine = DivinationEngine()
        
        # æ„å»ºå¦è±¡æ˜¾ç¤º
        hexagram_display = engine.format_hexagram_display(hexagram_result)
        
        # æ„å»ºå˜çˆ»æè¿°
        if analysis.changing_lines:
            changing_desc = f"å˜çˆ»ä½ç½®ï¼š{', '.join(str(p) for p in analysis.changing_lines)}çˆ»"
        else:
            changing_desc = "æ— å˜çˆ»ï¼ˆå…­çˆ»çš†é™ï¼‰"
        
        # å˜å¦ä¿¡æ¯
        bian_gua_info = ""
        if analysis.bian_gua:
            bian_gua_info = f"\n### å˜å¦ï¼š{analysis.bian_gua.full_name} {analysis.bian_gua.symbol}"
        
        prompt = f"""## æ±‚å¦ä¿¡æ¯
**é—®é¢˜**ï¼š{question}
**é—®äº‹ç±»å‹**ï¼š{question_type}
**èµ·å¦æ—¶é—´**ï¼š{hexagram_result.timestamp.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}

## å¦è±¡ä¿¡æ¯

### æœ¬å¦ï¼š{analysis.ben_gua.full_name} {analysis.ben_gua.symbol}

```
{hexagram_display}
```

{changing_desc}
{bian_gua_info}

## ä¸“ä¸šå…­çˆ»åˆ†æï¼ˆäº”ç»´åˆ†ææ³•ï¼‰

{analysis_context if analysis_context else "ï¼ˆæ— è¯¦ç»†åˆ†æï¼‰"}

## å‘¨æ˜“å…¸ç±å‚è€ƒ

{context}

---

è¯·æ ¹æ®ä»¥ä¸Š**äº”ç»´åˆ†æç»“æœ**å’Œå…¸ç±å‚è€ƒï¼Œä¸ºæ±‚å¦è€…è§£è¯»æ­¤å¦ã€‚

é‡ç‚¹å…³æ³¨ï¼š
1. ç”¨ç¥çš„æ—ºè¡°è¯„åˆ†ï¼Œè¿™å†³å®šäº†äº‹æƒ…çš„åŸºæœ¬èµ°å‘
2. åŠ¨çˆ»å¯¹ç”¨ç¥çš„å½±å“ï¼Œè¿™å†³å®šäº†äº‹æƒ…çš„å˜åŒ–è¶‹åŠ¿
3. ä¸–åº”å…³ç³»ï¼Œè¿™å†³å®šäº†åŒæ–¹çš„æ€åŠ¿

é’ˆå¯¹é—®é¢˜ã€Œ{question}ã€ç»™å‡ºä¸“ä¸šã€æœ‰æ®å¯ä¾çš„åˆ†æå’Œå»ºè®®ã€‚"""

        return prompt
    
    def generate(self, prompt: str) -> Generator[str, None, None]:
        """æµå¼ç”Ÿæˆè§£è¯»"""
        try:
            response = self.client.chat.completions.create(
                model=self.config["llm"]["model"],
                messages=[
                    {"role": "system", "content": self.SYSTEM_PROMPT},
                    {"role": "user", "content": prompt}
                ],
                temperature=self.config["llm"]["temperature"],
                max_tokens=self.config["llm"]["max_tokens"],
                stream=self.config["llm"]["stream"]
            )
            
            if self.config["llm"]["stream"]:
                for chunk in response:
                    if chunk.choices[0].delta.content:
                        yield chunk.choices[0].delta.content
            else:
                yield response.choices[0].message.content
                
        except Exception as e:
            yield f"\n\n[è§£è¯»ç”Ÿæˆå‡ºé”™: {str(e)}]"
    
    def generate_full(self, prompt: str) -> str:
        """ä¸€æ¬¡æ€§ç”Ÿæˆå®Œæ•´è§£è¯»"""
        return "".join(self.generate(prompt))
    
    def generate_followup(self, followup_question: str) -> Generator[str, None, None]:
        """ç”Ÿæˆè¿½é—®å›ç­”ï¼ˆä½¿ç”¨å¯¹è¯å†å²ï¼‰"""
        # å°†è¿½é—®åŠ å…¥å¯¹è¯å†å²
        self.conversation_history.append({
            "role": "user",
            "content": followup_question
        })
        
        try:
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = [{"role": "system", "content": self.FOLLOWUP_PROMPT}]
            messages.extend(self.conversation_history)
            
            response = self.client.chat.completions.create(
                model=self.config["llm"]["model"],
                messages=messages,
                temperature=self.config["llm"]["temperature"],
                max_tokens=800,  # è¿½é—®å›ç­”é™åˆ¶è¾ƒçŸ­
                stream=self.config["llm"]["stream"]
            )
            
            assistant_response = ""
            if self.config["llm"]["stream"]:
                for chunk in response:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        assistant_response += content
                        yield content
            else:
                assistant_response = response.choices[0].message.content
                yield assistant_response
            
            # å°†å›ç­”åŠ å…¥å¯¹è¯å†å²
            self.conversation_history.append({
                "role": "assistant",
                "content": assistant_response
            })
                
        except Exception as e:
            yield f"\n\n[è¿½é—®å›ç­”å‡ºé”™: {str(e)}]"
    
    def add_to_history(self, role: str, content: str):
        """å°†æ¶ˆæ¯åŠ å…¥å¯¹è¯å†å²"""
        self.conversation_history.append({
            "role": role,
            "content": content
        })


if __name__ == "__main__":
    # æµ‹è¯•
    client = LLMClient()
    
    from .divination_engine import DivinationEngine
    from .hexagram_analyzer import HexagramAnalyzer
    from .context_loader import ContextLoader
    
    engine = DivinationEngine()
    result = engine.cast_hexagram()
    
    analyzer = HexagramAnalyzer()
    analysis = analyzer.analyze(result)
    
    loader = ContextLoader()
    context = loader.load_context(analysis, "äº‹ä¸š")
    
    prompt = client.build_prompt(
        question="æœ€è¿‘å·¥ä½œä¼šæœ‰å˜åŠ¨å—ï¼Ÿ",
        question_type="äº‹ä¸š",
        hexagram_result=result,
        analysis=analysis,
        context=context
    )
    
    print("ç”Ÿæˆè§£è¯»ä¸­...\n")
    for text in client.generate(prompt):
        print(text, end="", flush=True)
    print()
